# -*- coding: utf-8 -*-
"""Assigment1-Vision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_yExdmy6C53_8hag4635aptcFbB5uSsG
"""

from google.colab import drive
drive.mount('/content/drive')

# do this just for the firt time
!unzip /content/drive/MyDrive/VisionProject/dataset.zip -d /content/drive/MyDrive/VisionProject/images/

import cv2
import numpy as np
from google.colab.patches import cv2_imshow
from matplotlib import pyplot as plt

"""# **TRACK A - Single Instance Detection**"""

def compute_SNR(image):
  # Convert the image to grayscale
    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

    # Compute the mean and standard deviation of the pixel values
    mean = np.mean(gray_image)
    std = np.std(gray_image)

    # Compute the SNR
    snr = mean / std
    return snr

def denoising(noisy_img):

  median_denoised = cv2.medianBlur(noisy_img, 3)
  nl_means_denoised = cv2.fastNlMeansDenoisingColored(median_denoised, None, 5, 5, 5, 21)
  bilateral_denoised = cv2.bilateralFilter(nl_means_denoised, 9, 75, 75)

  return bilateral_denoised

def match(des_query, des_train):
  # Defining index for approximate kdtree algorithm
  FLANN_INDEX_KDTREE = 1

  # Defining parameters for algorithm
  index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)

  # Defining search params.
  # checks=50 specifies the number of times the trees in the index should be recursively traversed.
  # Higher values gives better precision, but also takes more time
  search_params = dict(checks = 50)

  # Initializing matcher
  flann = cv2.FlannBasedMatcher(index_params, search_params)

  # Matching and finding the 2 closest elements for each query descriptor.
  matches = flann.knnMatch(des_query,des_train,k=2)

  good_matches = []
  threshold_distance = 0.65
  for m,n in matches:
    if m.distance < threshold_distance*n.distance:
      good_matches.append(m)

  return good_matches

from scipy import stats
# import seaborn as sns
def IQR_outlier(matchList):
  #az methode softmax estefade kon age khasti in tabaro modify koni
  return abfiltered_df

MIN_MATCH_COUNT = 60

# Initiate SIFT detector
sift = cv2.xfeatures2d.SIFT_create()

# Load reference images
reference_images = []
reference_keypoints = []
reference_descriptors = []

for i in range(1, 15):
  img = cv2.imread(f'drive/MyDrive/VisionProject/images/dataset/models/ref{i}.png', cv2.IMREAD_GRAYSCALE)
  reference_images.append(img)
  kp, des = sift.detectAndCompute(img, None)
  reference_keypoints.append(kp)
  reference_descriptors.append(des)

for i in range(1, 6):
  img_train = cv2.imread(f'drive/MyDrive/VisionProject/images/dataset/scenes/scene{i}.png')
  img_train_d = cv2.cvtColor(denoising(img_train), cv2.COLOR_BGR2GRAY)
  # img_train_d = denoising(img_train)

  kp_train = sift.detect(img_train_d)
  kp_train, des_train = sift.compute(img_train_d, kp_train)
  # check_matches = []
  # bounding_box = {}
  for k in range(len(reference_images)):
    good_matches = match(reference_descriptors[k], des_train)

    if len(good_matches) > MIN_MATCH_COUNT:
      print(f"scene{i} in ref{k+1} - number of matches: {len(good_matches)}")
      # building the corrspondences arrays of good matches
      src_pts = np.float32([ reference_keypoints[k][m.queryIdx].pt for m in good_matches ]).reshape(-1,1,2)
      dst_pts = np.float32([ kp_train[m.trainIdx].pt for m in good_matches ]).reshape(-1,1,2)

      # Using RANSAC to estimate a robust homography.
      # It returns the homography M and a mask for the discarded points
      M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 4.0)

      # Mask of discarded point used in visualization
      matchesMask = mask.ravel().tolist()
      print("matchesMAsk : ",sum(matchesMask))
      # Corners of the query image
      h,w = reference_images[k].shape
      pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)

      # Projecting the corners into the train image
      dst = cv2.perspectiveTransform(pts,M)

      # Drawing the bounding box
      # check_matches.append(len(good_matches))
      # bounding_box[len(good_matches)] = np.int32(dst)
      cv2.polylines(img_train,[np.int32(dst)],True,((9*(k + 1) % 255), 255, (5*(k + 1) % 255)),7, cv2.LINE_AA)

    else:
      print(f"Not enough matches are found - scene{i} in ref{k+1} - number of matches: {len(good_matches)}")
  # mt = IQR_outlier(check_matches)
  # print("update : ............. ", mt)
  # for i in range(len(mt)):
  #   cv2.polylines(img_train,[bounding_box[mt[i]]],True,((9*(k + 1) % 255), 255, (5*(k + 1) % 255)),7, cv2.LINE_AA)
  plt.imshow(cv2.cvtColor(img_train, cv2.COLOR_BGR2RGB))
  plt.show()