# -*- coding: utf-8 -*-
"""Assigment1-Vision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_yExdmy6C53_8hag4635aptcFbB5uSsG
"""

# from google.colab import drive
# drive.mount('/content/drive')

# # do this just for the firt time
# !unzip /content/drive/MyDrive/VisionProject/dataset.zip -d /content/drive/MyDrive/VisionProject/images/

import os
import cv2
import numpy as np
# from google.colab.patches import cv2_imshow
from matplotlib import pyplot as plt

def denoising(noisy_img):

  median_denoised = cv2.medianBlur(noisy_img, 3)
  nl_means_denoised = cv2.fastNlMeansDenoisingColored(median_denoised, None, 5, 5, 5, 21)
  bilateral_denoised = cv2.bilateralFilter(nl_means_denoised, 9, 75, 75)

  return bilateral_denoised

def match(des_query, des_train):
  # Defining index for approximate kdtree algorithm
  FLANN_INDEX_KDTREE = 1

  # Defining parameters for algorithm
  index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)

  # Defining search params.
  # checks=50 specifies the number of times the trees in the index should be recursively traversed.
  # Higher values gives better precision, but also takes more time
  search_params = dict(checks = 50)

  # Initializing matcher
  flann = cv2.FlannBasedMatcher(index_params, search_params)

  # Matching and finding the 2 closest elements for each query descriptor.
  matches = flann.knnMatch(des_query,des_train,k=2)

  good_matches = []
  threshold_distance = 0.65
  for m,n in matches:
    if m.distance < threshold_distance*n.distance:
      good_matches.append(m)

  return good_matches

MIN_MATCH_COUNT = 60

# Initiate SIFT detector
sift = cv2.xfeatures2d.SIFT_create()

# Load reference images
reference_images = []
reference_keypoints = []
reference_descriptors = []

for i in range(1, 15):
  img = cv2.imread(f'dataset/models/ref{i}.png', cv2.IMREAD_GRAYSCALE)
  reference_images.append(img)
  kp, des = sift.detectAndCompute(img, None)
  reference_keypoints.append(kp)
  reference_descriptors.append(des)

for i in range(1, 2):
  img_train = cv2.imread(f'dataset/scenes/scene{i}.png')
  img_train_d = cv2.cvtColor(denoising(img_train), cv2.COLOR_BGR2GRAY)

  kp_train = sift.detect(img_train_d)
  kp_train, des_train = sift.compute(img_train_d, kp_train)
  
  for k in range(len(reference_images)):
    good_matches = match(reference_descriptors[k], des_train)

    if len(good_matches) > MIN_MATCH_COUNT:
      print(f"scene{i} in ref{k+1} - number of matches: {len(good_matches)}")
      # building the corrspondences arrays of good matches
      src_pts = np.float32([ reference_keypoints[k][m.queryIdx].pt for m in good_matches ]).reshape(-1,1,2)
      dst_pts = np.float32([ kp_train[m.trainIdx].pt for m in good_matches ]).reshape(-1,1,2)

      # Using RANSAC to estimate a robust homography.
      # It returns the homography M and a mask for the discarded points
      M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 4.0)

      # Mask of discarded point used in visualization
      matchesMask = mask.ravel().tolist()
      print("matchesMAsk : ",sum(matchesMask))
      # Corners of the query image
      h,w = reference_images[k].shape
      pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)

      # Projecting the corners into the train image
      dst = cv2.perspectiveTransform(pts,M)

      # Drawing the bounding box
      cv2.polylines(img_train,[np.int32(dst)],True,((9*(k + 1) % 255), 255, (5*(k + 1) % 255)),7, cv2.LINE_AA)

    else:
      print(f"Not enough matches are found - scene{i} in ref{k+1} - number of matches: {len(good_matches)}")

  plt.imshow(cv2.cvtColor(img_train, cv2.COLOR_BGR2RGB))
  plt.show()